{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification baseline\n",
    "This notebook contains the code for our final multi-modal model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import from_numpy\n",
    "from torch.nn import MSELoss, Linear, ReLU, Dropout, Sequential\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import imageio.v3 as imageio\n",
    "\n",
    "from lightning import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "seed_everything(42)\n",
    "\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from timm import create_model\n",
    "\n",
    "import wandb\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data\"\n",
    "IMAGES_PATH = BASE_PATH + \"/train_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Custom Dataset\n",
    "We refined the custom dataset from the image model, incorporating the tabular data. The dataset scales the tabular data on creation and returns the image and tabular data when indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root, dataset_path, transforms):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        dataset = pd.read_csv(dataset_path)\n",
    "        dataset[\"file_path\"] = root + \"/\" + dataset[\"id\"].astype(str) + \".jpeg\"\n",
    "        self.image_data = dataset[\"file_path\"].apply(lambda fp: open(fp, \"rb\").read())\n",
    "\n",
    "        self.target_columns = [\"X4\", \"X11\", \"X18\", \"X26\", \"X50\", \"X3112\"]\n",
    "        if self.target_columns[0] not in dataset.columns:\n",
    "            self.target_columns = [f\"{col}_mean\" for col in self.target_columns]\n",
    "\n",
    "        y = np.zeros_like(dataset[self.target_columns], dtype=np.float32)\n",
    "        for target_idx, target in enumerate(self.target_columns):\n",
    "            v = dataset[target].values\n",
    "            if target in self.target_columns:\n",
    "                v = np.log10(v)\n",
    "            y[:, target_idx] = v\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.y = self.scaler.fit_transform(y)\n",
    "\n",
    "        self.tab_data = dataset.drop(columns=[\"id\", \"file_path\"] + self.target_columns + [col for col in dataset if col.endswith('_sd')])\n",
    "        for column in self.tab_data.columns:\n",
    "            min_val = self.tab_data[column].min()\n",
    "            max_val = self.tab_data[column].max()\n",
    "            self.tab_data[column] = (self.tab_data[column] - min_val) / (max_val - min_val)  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tab_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.transforms(image=imageio.imread(self.image_data[index]))[\"image\"]\n",
    "        tab = torch.tensor(self.tab_data.iloc[index].values, dtype=torch.float32)\n",
    "        targets = from_numpy(self.y[index])\n",
    "        \n",
    "        return image, tab, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Model\n",
    "Our multi-modal model loads the swin transformer, gets the output dimensions of it and then creates both the tabular data network and the regressor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        image_dropout,\n",
    "        image_learning_rate_modifier,\n",
    "        image_decay,\n",
    "        tab_features,\n",
    "        tab_hidden_dim,\n",
    "        tab_activation,\n",
    "        tab_dropout,\n",
    "        tab_decay,\n",
    "        regressor_hidden_dim,\n",
    "        regressor_activation,\n",
    "        regressor_dropout,\n",
    "        regressor_n_layers,\n",
    "        regressor_decay,\n",
    "        learning_rate,\n",
    "        n_steps,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.image_model = Sequential(\n",
    "            create_model(model_name, num_classes=0, pretrained=True),\n",
    "            Dropout(image_dropout),\n",
    "        )\n",
    "\n",
    "        image_model_output_shape = self.image_model[0].norm.normalized_shape[0]\n",
    "\n",
    "        self.tab_model = Sequential(\n",
    "            Linear(tab_features, tab_hidden_dim),\n",
    "            tab_activation(),\n",
    "            Dropout(tab_dropout),\n",
    "            Linear(tab_hidden_dim, image_model_output_shape),\n",
    "        )\n",
    "\n",
    "        concat_size = image_model_output_shape * 2\n",
    "\n",
    "        self.regressor = Sequential()\n",
    "        for i in range(self.hparams.regressor_n_layers):\n",
    "            self.regressor.append(Linear(concat_size, concat_size))\n",
    "            self.regressor.append(regressor_activation())\n",
    "            self.regressor.append(Dropout(regressor_dropout))\n",
    "        self.regressor.append(Linear(concat_size, 6))\n",
    "        \n",
    "        self.loss = MSELoss()\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        image, tab, y = batch\n",
    "        image_output = self.image_model(image)\n",
    "\n",
    "        tab_output = self.tab_model(tab)\n",
    "\n",
    "        combined = torch.cat([image_output, tab_output], dim=1)\n",
    "\n",
    "        output = self.regressor(combined)\n",
    "        loss = self.loss(output, y)\n",
    "\n",
    "        metric = R2Score()\n",
    "        metric.update(output, y)\n",
    "        \n",
    "        return loss, metric.compute()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, r2 = self(batch)\n",
    "        self.log(\"Train loss\", loss, prog_bar=True)\n",
    "        self.log(\"Train R2\", r2, prog_bar=True)\n",
    "\n",
    "        self.lr_schedulers().step()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch):\n",
    "        loss, r2 = self(batch)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation R2\", r2)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            params=self.image_model.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.image_decay\n",
    "        )\n",
    "\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            max_lr=self.hparams.learning_rate,\n",
    "            total_steps=self.hparams.n_steps,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos',\n",
    "            div_factor=1e1,\n",
    "            final_div_factor=1e1,\n",
    "        )\n",
    "\n",
    "        return [optimizer], [scheduler] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Training\n",
    "### 4.1 - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(config={\n",
    "    \"model_name\": \"swin_large_patch4_window12_384.ms_in22k_ft_in1k\",\n",
    "    \"image_dropout\": 0,\n",
    "    \"image_learning_rate_modifier\": 1/10,\n",
    "    \"image_decay\": 0.01,\n",
    "    \"tab_hidden_dim\": 512,\n",
    "    \"tab_activation\": \"ReLU\",\n",
    "    \"tab_dropout\": 0,\n",
    "    \"tab_decay\": 0,\n",
    "    \"regressor_hidden_dim\": 512,\n",
    "    \"regressor_activation\": \"ReLU\",\n",
    "    \"regressor_dropout\": 0,\n",
    "    \"regressor_decay\": 0,\n",
    "    \"regressor_n_layers\": 1,\n",
    "    \"learning_rate\": 1e-4,\n",
    "}, project=\"aicomp\") as run:\n",
    "    wandb.define_metric(\"Validation R2\", summary=\"max\")\n",
    "    config = wandb.config\n",
    "    image_size = 384\n",
    "    \n",
    "    transforms = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomSizedCrop(\n",
    "            [448, 512],\n",
    "            image_size, image_size, w2h_ratio=1.0, p=0.75),\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n",
    "        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n",
    "        A.ToFloat(),\n",
    "        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    dataset = MultiModalDataset(BASE_PATH + \"/train_images\", BASE_PATH + \"cleaned/cleaned_train.csv\", transforms=transforms)\n",
    "    train, val = random_split(dataset, [0.9, 0.1])\n",
    "\n",
    "    max_epochs = 10\n",
    "    batch_size = 10\n",
    "    n_steps = ((len(dataset) // batch_size) * max_epochs) + 1\n",
    "\n",
    "    tab_activation = ReLU if config[\"tab_activation\"] else None\n",
    "    regressor_activation = ReLU if config[\"regressor_activation\"] else None\n",
    "    \n",
    "    model = MultiModel(\n",
    "        model_name=config[\"model_name\"],\n",
    "        image_dropout=config[\"image_dropout\"],\n",
    "        image_learning_rate_modifier=config[\"image_learning_rate_modifier\"],\n",
    "        image_decay=config[\"image_decay\"],\n",
    "        tab_features=dataset.tab_data.shape[1],\n",
    "        tab_hidden_dim=config[\"tab_hidden_dim\"],\n",
    "        tab_activation=tab_activation,\n",
    "        tab_dropout=config[\"tab_dropout\"],\n",
    "        tab_decay=config[\"tab_decay\"],\n",
    "        regressor_hidden_dim=config[\"regressor_hidden_dim\"],\n",
    "        regressor_activation=regressor_activation,\n",
    "        regressor_dropout=config[\"regressor_dropout\"],\n",
    "        regressor_decay=config[\"regressor_decay\"],\n",
    "        regressor_n_layers=config[\"regressor_n_layers\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        n_steps=n_steps\n",
    "    )\n",
    "\n",
    "    logger = WandbLogger(project=\"aicomp\", log_model=\"all\")\n",
    "\n",
    "    callbacks = [ModelCheckpoint(monitor=\"Validation R2\", mode=\"max\")]\n",
    "    callbacks += [EarlyStopping(monitor=\"Validation loss\", mode=\"min\", patience=5)]\n",
    "    callbacks += [EarlyStopping(monitor=\"Validation R2\", mode=\"max\", patience=5)]\n",
    "    callbacks += [EarlyStopping(monitor=\"Train loss\", mode=\"min\", patience=5, stopping_threshold=0.01, check_on_train_epoch_end=True)]\n",
    "    callbacks += [EarlyStopping(monitor=\"Train R2\", mode=\"max\", patience=5, stopping_threshold=0.99, check_on_train_epoch_end=True)]\n",
    "    callbacks += [LearningRateMonitor(logging_interval=\"step\")]\n",
    "    \n",
    "    trainer = Trainer(max_epochs=max_epochs, logger=logger, num_sanity_val_steps=0, callbacks=callbacks)\n",
    "    \n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
    "    val_dataloader = DataLoader(val, batch_size=batch_size, num_workers=batch_size)\n",
    "    \n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    del model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.teardown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
